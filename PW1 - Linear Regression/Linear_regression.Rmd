---
title: "Linear_regression"
output: html_document
---

# Practical Work 2
```{r}
setwd("C:/Users/flavi/OneDrive/Documents/ESILV/A4/S1/Machine Learning/Machine_learning/PW1 - Linear Regression")
```

## Multiple Linear Regression
### 1)  Load the Boston dataset from MASS package
```{r}
# load MASS package
library(MASS)

# Check the dimensions of the Boston dataset
dim(Boston)
```

### 2) Split the dataset into traning set and testing set. (keep all the variables of the Boston data set)
```{r}
# We use the first 80% observations as the training
train = 1:as.integer(dim(Boston)[1]*0.80)
test = -train

training_data = Boston[train,]
testing_data = Boston[test,]

dim(training_data)
dim(testing_data)
```

### 3) Check if there is a linear relationship between the variables medv and age. (use cor() function)
We plot age vs medv (response) to have a first guess of the answer
(not a proof)   
```{r}
plot(training_data$age, training_data$medv,
     xlab="age", ylab="medv",
     main="Scatterplot of age vs. medv",
     col="blue", type="p",pch = 20)
```

Then, we compute the correlations between age (a predictor) and medv (the response)
```{r}
# Syntax of the function cor() :
# corelation = cor(training_data, y = NULL, use = "everything",
#                 method = c("pearson", "kendall", "spearman"))

cor(training_data$age, training_data$medv,method = "pearson")
```

Interpretation:   
If the correlation is :   
-> near 1 : strong positive correlation   
-> near -1 : strong negative correlation   
-> near 0 : no correlation at all   
   
The result here is -0.2924247 and it's closer to 0. We have a weak negative correlation between the predictor age and the response medv.   
   
My interpretation:   
Intuitively, one can think that the age of the owner does not necessarily have a strong influence on the price of the property. Indeed, the professional activity or the family patrimony allows younger people to afford a house of the same value as an older person. 


### 4) Fit a model of housing prices in function of age and plot the observations and the regression line
First, we run the linear regression model
```{r}
# Syntax of the function lm (linear model):
# model = lm(formula = response ~ predictor, data = data)
model_age = lm(medv ~ age, data=training_data)
model_age
```

We plot the observations and the regression line.  
```{r}
plot(training_data$age, training_data$medv,
     xlab = "Age of the proprietary",
     ylab = "Median House Value",
     col = "red",
     main = "Scatterplot of age vs. medv with the linear regression line",
     pch = 20)
abline(model_age, col = "blue", lwd="2")
```
   
As infer above, there is a small correlation but the variability of the data is important.

### 5) Train a regression model using both lstat and age as predictors of median house value.
Now, we are going to use lstat and age as features to predict the medv target
In the previous practical work, we check for linearity between lstat and medv and we come to the conclusion that we need to use a logarithm scale to get a stronger linear relationship:  
```{r}
plot(training_data$lstat, training_data$medv,
     xlab="lStat", ylab="medv",
     main="Scatterplot of lstat vs. medv",
     col="blue", type="p",pch = 20)
print(cor(training_data$lstat, training_data$medv))
```
```{r}
print(cor(log(training_data$lstat), training_data$medv))
plot(log(training_data$lstat), training_data$medv,
     xlab="lStat", ylab="medv",
     main="LogScatterplot of lstat vs. medv",
     col="blue", type="p",pch = 20)
```
  
Correlation coefficient before the log transformation between lstat and medv = -0.7085914  
Correlation coefficient after the log transformation between lstat and medv = -0.7961729  
  
**Note:** We can't do the same transformation for the age predictor because the weak linearship relation comes from the data variability 

```{r}
model_age_lstat = lm(medv ~ age+log(lstat), data=training_data)
model_age_lstat
```

### 6) Summary of the model
```{r}
# Print the summary of the obtained regression model :
summary(model_age_lstat)
```

### 7) Is the model as a whole significant?
There are two parameters to take into account for the whole significance:  
- P-value for the entire model compute with the Fisher test performed by R
  We found a p-value < 2.2e-16,   
  So p-value < 0.05 => Since the p-value is less than the significance level (5%), we can conclude that our regression model fits the data better than the intercept-only model. So there is a significant relationship between the predictors (house owner age and  percentage of households with low socioeconomic income) and the response (median house value)    
- F-statistic: The F value should always be used along with the p value in deciding whether your results are significant enough to reject the null hypothesis. Let's calculate the F Critical Value   
```{r}
qf(p=.05, df1=2, df2=401) 
# where :
#p: The significance level to use
#df1: The numerator degrees of freedom
#df2: The denominator degrees of freedom
```
So F-statistic = 411.9 > F critical value 0.0512, so that's confirming the p-value assumption
  
Thus, we can say that the model as a whole significant

Adjusted r-squared : closer to 1 : high proportion are explain by the input, 67,1% of variability is explained


### 8) Predictors significant
We use the Student tests that were performed by R   
- age:  
  - t-value = 4.082 (p-value < 0.05) : significant relationship between the owner age and the median house value.   
- lstat:   
  - t-value = -19.134 (p-value < 0.05): significant relationship between the percentage of households with low socioeconomic income and the median house value.  
  
The predictors are both significant.  

### 9) Model with all variables of the Boston dataset
```{r}
model_all_variables = lm(medv~., data=training_data)
model_all_variables
```

### 10) Model with all variables (but log(lstat)) of the Boston dataset
```{r}
model_all_variables_log_lstat = lm(medv~.-lstat+log(lstat), data=training_data)
model_all_variables_log_lstat
```

### 11) Did $R^2$ improve ?
```{r}
summary(model_all_variables)$r.squared
summary(model_all_variables_log_lstat)$r.squared
```
The $R^2$ squared improved a lot when we use log(stat) instead of stat

### 12) To see if there is correlated variables print the correlation matrix
```{r}
round(cor(training_data),2)
```

### 13) Visualize the correlations
```{r}
#install.packages("corrplot")
```
```{r}
library(corrplot)
```

```{r}
M = cor(training_data)
corrplot.mixed(M)
```

### 14) Correlation between tax and rad   
corr = 0.88 high, near 1   
If the result is different that comes from I took the 80% first values (404 values) instead of the 400 first ones 

Tax and rad are very positively correlated

### 15) Run the model again without tax. What happens to the R2 and for the F-statistic?
```{r}
model_without_tax = lm(medv~.-(tax+lstat)+log(lstat), data=training_data)
summary(model_all_variables_log_lstat)$r.squared
summary(model_without_tax)$r.squared
summary(model_all_variables_log_lstat)$fstatistic["value"]
summary(model_without_tax)$fstatistic["value"]
```
F-statistic = 109.8335   
F-statistic without tax = 114.3025   
   
$R^2$ = 0.7854591   
$R^2$ without tax = 0.7781724   
   
R^2 decreased because we deleted one of the variable. However, the F-statistic increased, which means the p-values gets lower and thus the model is more significant without tax.

### 16) MSE
```{r}
y = testing_data$medv

# Compute the predicted value for this y (y hat)
y_hat = predict(model_without_tax, data.frame(testing_data))

# Now we have both y and y_hat for our testing data. 
# let's find the mean square error
error = y-y_hat
error_squared = error^2
MSE = mean(error_squared)
MSE
```
## ANOVA
### 17) In the Boston data set there is a categorical variable chas which corresponds to Charles River (= 1 if a suburb bounds the river; 0 otherwise). Use command str() to see how this variable is present in the dataset. How many of the suburbs in this data set bound the Charles river?
```{r}
str(Boston$chas)
table(Boston$chas)
sum(Boston$chas)
```
There are 35 suburbs in this data set bound the Charles river.

### 18) Do we observe some difference between the median value of houses with respect to the neighborhood to Charles River?  
```{r}
boxplot(Boston$medv ~ Boston$chas)
```
  
Observation :  
The house near the river have a higher median value but the not really significant (and also a much higher quartiles 1 and 3, minimum and maximum).


### 19) Calculate $μ_i$ and $μ_j$
```{r}
aggregate(Boston$medv, list(Boston$chas),FUN=mean)
```
 We observe a much higher mean price if near river (28,44 vs 22.09).

### 20) Apply an ANOVA test of medv whith respect to chas (use the function aov())
```{r}
anova = aov(medv~chas, data=Boston)
anova
```
```{r}
summary(anova)
```
p = 7.39e-05 < 0.05 (threshold of significance)   
We can reject the null hypothesis and conclude that there is a significative difference of price depending of the proximity of the river.

## Qualitative predictors
### 21) Fit a new model where the predictors are the Charles River and the Crime Rate. Interpret the coefficients of this model and conclude if the presence of the river adds a valuable information for explaining the house price. 
```{r}
model_crim_chas = lm(medv~crim+chas, data=training_data)
model_crim = lm(medv~crim, data=training_data)
model_crim_chas
model_crim
```
```{r}
summary(model_crim)$fstatistic["value"]
summary(model_crim_chas)$fstatistic["value"]
summary(model_crim)$r.squared
summary(model_crim_chas)$r.squared
```

```{r}
summary(model_crim)
summary(model_crim_chas)
```
F-statistic - model with crim: 39.22707   
F-statistic - model with crim and chas: 24.47648   
Adjusted $R^2$ - model with crim: 0.08890449  
Adjusted $R^2$ - model with crim and chas: 0.1087957  

```{r}
print(cor(training_data$chas, training_data$medv))
plot(training_data$chas, training_data$medv,
     xlab="chas", ylab="medv", main = "Scatterplot of medv vs chas",
     col="blue", type="p",pch = 20)
```

```{r}
print(cor(training_data$crim, training_data$medv))
plot(training_data$crim, training_data$medv,
     xlab="crim", ylab="medv", main = "Scatterplot of medv vs crim",
     col="blue", type="p",pch = 20)
```
  
The adjusted $R^2$ is a little bit higher with the presence of chas (because we add a variable to the model) but the F-statistic and the p-value is lower with the chas variable.  
Despite everything, p-value for chas equals to 0.00295 which is lower than the 5% threshold.  
The presence of the river doesn't add a valuable information for explaining the house price.

### 22) Is chas significant as well in the presence of more predictors?
```{r}
model_all = lm(medv~., data=training_data)
model_all_without_chas = lm(medv~.-chas, data=training_data)
summary(model_all)
summary(model_all_without_chas)
```
For the same reasons, chas is still insignificant (F-statistic higher without chas,...)

## Interaction terms
### 23) Fit a model with first order interaction term where predictors are lstat and age. Print its summary.
```{r}
model_interaction_terms = lm(medv~lstat:age, data=training_data)
summary(model_interaction_terms)
model_age_lstat = lm(medv~lstat+age, data=training_data)
summary(model_age_lstat)
```

### 24) Fit a model with all the first order interaction terms
```{r}
model_age_lstat = lm(medv~lstat*age, data=training_data)
summary(model_age_lstat)
```
With the questions 23 and 24, we can say that the term $X_1X_2$ on itself doesn't give us the best predictions ($R^2$ lower) but by adding it to the other terms $X_1$ and $X_2$, we get really good predictions.